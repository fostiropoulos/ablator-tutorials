{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started with ABLATOR ðŸš€ and Dashboard\n",
    "\n",
    "Dashboard is an experimental feature for visualizing, storing and monitoring your experiments remotely. \n",
    "\n",
    "## Pre-requisites\n",
    "\n",
    "You will need a remote storage credentials set-up with a 3rd-party provider such as AWS, GCP or you can even set-up your own SSH server. You can find more about how to set-up your own personal remote storage [HERE](https://github.com/fostiropoulos/rmount)\n",
    "\n",
    "### fuse3\n",
    "\n",
    "ABLATOR uses [rmount](https://github.com/fostiropoulos/rmount) in the background to create a Fuse file-system mountpoint that automatically synchronizes the contents\n",
    "\n",
    "```bash\n",
    "$ apt-get install fuse3\n",
    "```\n",
    "\n",
    "### WARNING\n",
    "\n",
    "Distributed applications have many moving parts that all have to play nice together for them to work. There are a number of things that can go wrong when trying to synchronize your file system and experiments with a remote storage and are also impossible to mitigate from the ABLATOR end. \n",
    "\n",
    "This feature is experimental and you should expect to encounter errors when using it. Reporting errors, issues and sharing your story with us can help us improve our library [HERE](https://github.com/fostiropoulos/ablator/issues)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!apt-get install fuse3\n",
    "try:\n",
    "    import ablator\n",
    "except ImportError:\n",
    "    !pip install ablator\n",
    "    print(\"Stopping RUNTIME! Please run again.\")\n",
    "    import os\n",
    "\n",
    "    os.kill(os.getpid(), 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of S3 Configuration\n",
    "\n",
    "```python\n",
    "\n",
    "remote_config = RemoteConfig(\n",
    "    s3={\n",
    "        \"provider\": \"AWS\", # or GCP\n",
    "        \"region\": \"us-east-1\",\n",
    "        \"access_key_id\": \"xxx\",\n",
    "        \"secret_access_key\": \"xxx\",\n",
    "    },\n",
    "    remote_path=Path(\"some-bucket\"), # s3://some-bucket (you must create it in your account settings)\n",
    ")\n",
    "```\n",
    "\n",
    "SSH configuration requires additional tinkering and is **NOT** recommended for first-time users.\n",
    "\n",
    "\n",
    "To find out how to set-up secret credentials you can use the official guide for [AWS](https://aws.amazon.com/blogs/security/wheres-my-secret-access-key/) and [GCP](https://cloud.google.com/storage/docs/authentication/managing-hmackeys). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import shutil\n",
    "from torch import nn\n",
    "import torch\n",
    "from ablator import (\n",
    "    ModelConfig,\n",
    "    ModelWrapper,\n",
    "    OptimizerConfig,\n",
    "    TrainConfig,\n",
    "    configclass,\n",
    "    Literal,\n",
    "    ParallelTrainer,\n",
    "    SearchSpace,\n",
    "    ParallelConfig,\n",
    "    RemoteConfig,\n",
    ")\n",
    "\n",
    "remote_config = RemoteConfig(\n",
    "    s3={\n",
    "        \"provider\": \"AWS\",\n",
    "        \"region\": \"us-east-2\",\n",
    "        \"access_key_id\": \"xxx\",\n",
    "        \"secret_access_key\": \"xxx\",\n",
    "    },\n",
    "    remote_path=Path(\"ablator-mock\"),  # s3://some-bucket\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Experiment\n",
    "\n",
    "We set-up a simple experiment where we ablate the use of two layer types\n",
    "\n",
    "#### Layer-A\n",
    "```python\n",
    "self.param = nn.Parameter(torch.ones(50, 1))\n",
    "```\n",
    "#### Layer-B\n",
    "\n",
    "```python\n",
    "self.param = nn.Parameter(torch.randn(200, 1))\n",
    "```\n",
    "\n",
    "\n",
    "There are two things to note, our simple experiment modifies the size of the layer i.e. `50` vs `200` and the initialization i.e. `randn` (random) vs `ones`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@configclass\n",
    "class TrainConfig(TrainConfig):\n",
    "    dataset: str = \"random\"\n",
    "    dataset_size: int\n",
    "\n",
    "\n",
    "@configclass\n",
    "class ModelConfig(ModelConfig):\n",
    "    layer: Literal[\"layer_a\", \"layer_b\"] = \"layer_a\"\n",
    "\n",
    "\n",
    "@configclass\n",
    "class ParallelConfig(ParallelConfig):\n",
    "    model_config: ModelConfig\n",
    "    train_config: TrainConfig\n",
    "\n",
    "\n",
    "config = ParallelConfig(\n",
    "    experiment_dir=Path(\"/tmp/ablator-exp\"),\n",
    "    train_config=TrainConfig(\n",
    "        batch_size=128,\n",
    "        epochs=2,\n",
    "        dataset_size=100,\n",
    "        optimizer_config=OptimizerConfig(name=\"sgd\", arguments={\"lr\": 0.1}),\n",
    "        scheduler_config=None,\n",
    "    ),\n",
    "    model_config=ModelConfig(),\n",
    "    device=\"cpu\",\n",
    "    search_space={\n",
    "        \"model_config.layer\": SearchSpace(categorical_values=[\"layer_a\", \"layer_b\"])\n",
    "    },\n",
    "    total_trials=10,\n",
    "    remote_config=remote_config,\n",
    ")\n",
    "\n",
    "\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self, config: ModelConfig) -> None:\n",
    "        super().__init__()\n",
    "        if config.layer == \"layer_a\":\n",
    "            self.param = nn.Parameter(torch.ones(50, 1))\n",
    "        else:\n",
    "            self.param = nn.Parameter(torch.randn(200, 1))\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = self.param * x\n",
    "        return {\"preds\": x}, x.sum().abs()\n",
    "\n",
    "\n",
    "class SimpleWrapper(ModelWrapper):\n",
    "    def make_dataloader_train(self, run_config: ParallelConfig):\n",
    "        dl = [torch.rand(100) for i in range(run_config.train_config.dataset_size)]\n",
    "        return dl\n",
    "\n",
    "    def make_dataloader_val(self, run_config: ParallelConfig):\n",
    "        dl = [torch.rand(100) for i in range(run_config.train_config.dataset_size)]\n",
    "        return dl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To the stars! ðŸš€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    mywrapper = SimpleWrapper(SimpleModel)\n",
    "    shutil.rmtree(config.experiment_dir, ignore_errors=True)\n",
    "    with ParallelTrainer(mywrapper, config) as runner:\n",
    "        runner.launch(\".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Dashboard (even while the experiment is running)\n",
    "\n",
    "[Dashboard](https://dashboard.ablator.org)\n",
    "\n",
    "Create an experiment and follow the instructions\n",
    "![Create an Experiment](assets/create_experiment.png)\n",
    "\n",
    "![Step 1](assets/step_1.png)\n",
    "\n",
    "### Compare Trial Configurations\n",
    "\n",
    "![Configuration Heading](assets/config_bar.png)\n",
    "![Configuration Comparison](assets/configurations.png)\n",
    "\n",
    "### Review Logs\n",
    "\n",
    "![Logs Heading](assets/logs_bar.png)\n",
    "![Logs](assets/logs.png)\n",
    "\n",
    "\n",
    "### Review Performance\n",
    "\n",
    "![Time-Series Heading](assets/timeseries_bar.png)\n",
    "\n",
    "![Results](assets/timeseries.png)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
