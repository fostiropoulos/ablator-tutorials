{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QZtMkXr2vBqs"
   },
   "source": [
    "# Explore hyperparameter tuning with ABLATOR\n",
    "\n",
    "In this demo, we are gonna train a LeNet model with MNIST dataset with Ablator under Colab enviroments. After that, we will also use the Multi-processing feature of Ablator's to tune different hyperparameters for our model and compare the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k_mMoS5mvsaI"
   },
   "source": [
    "## Prerequisites\n",
    "\n",
    "Before we start, please enable GPU on your CoLab. Please select the `Change Runtime type` option from the CoLab toolbar and choose `GPU` as the hardware accelerator.\n",
    "\n",
    "To start with, we can clone the Ablator repository from Github to our CoLab workspace:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OOUZdwtYvnp8"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    import ablator\n",
    "except ImportError:\n",
    "    !pip install git+https://github.com/fostiropoulos/ablator.git@v0.0.1-misc-fixes\n",
    "    print(\"Stopping RUNTIME! Please run again.\")\n",
    "    import os\n",
    "\n",
    "    os.kill(os.getpid(), 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KqfnhrzSwXvn"
   },
   "source": [
    "Then we install all the dependecies.\n",
    "\n",
    "Please note that: Since there are some package version conflicts, the installation process is seperated. We will fix this problem later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YXAgFLsO0TIz"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import Callable, Dict\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch import nn\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dMv5LTHs0lNE"
   },
   "outputs": [],
   "source": [
    "from ablator import (\n",
    "    ModelConfig,\n",
    "    ModelWrapper,\n",
    "    RunConfig,\n",
    "    configclass,\n",
    "    Literal,\n",
    "    ParallelConfig,\n",
    "    ParallelTrainer,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i2fmpHLI3giz"
   },
   "source": [
    "## Set up basic configurations\n",
    "\n",
    "First thing for using Ablator is to set up the configurations, including the model configurations and the training configurations.\n",
    "\n",
    "Since we are running Ablator on CoLab, we define the inline parameters directly and use **NO** configuration files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5q-A4_El7W5e"
   },
   "outputs": [],
   "source": [
    "# Customized model config subclass, inheriting from ModelConfig base class\n",
    "@configclass\n",
    "class LenetConfig(ModelConfig):\n",
    "    # Configurable attributes\n",
    "    name: Literal[\"lenet5\"]\n",
    "\n",
    "\n",
    "# Customized Run config subclass, inheriting from RunConfig base class\n",
    "@configclass\n",
    "class LenetRunConfig(RunConfig):\n",
    "    model_config: LenetConfig\n",
    "\n",
    "\n",
    "# Customized Parallel Training config class, inheriting from ParallelConfig base class\n",
    "@configclass\n",
    "class MyParallelConfig(ParallelConfig):\n",
    "    model_config: LenetConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a4-rKWTZFj2Y"
   },
   "source": [
    "Then we create objects for each necessary configuration classes and fill in the configuration values into them.\n",
    "\n",
    "In this demo, we implemented these configration objects:\n",
    "\n",
    "*   `TrainConfig`: Training parameters, including the dataset, epochs number, batch size and optimizer etc.\n",
    "*   `ModelConfig`: Specify the model class we are gonna try\n",
    "*   `ParellelConfig`: Experiments parameters, including the dir, other config's class, trials, hyperparameter search space and hardware related parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kq7ubuq2Fsmz"
   },
   "outputs": [],
   "source": [
    "from ablator import OptimizerConfig, TrainConfig\n",
    "from ablator.config.hpo import SearchSpace\n",
    "\n",
    "# Define the training configuration object\n",
    "train_config = TrainConfig(\n",
    "    dataset=\"mnist\",\n",
    "    batch_size=64,\n",
    "    epochs=10,\n",
    "    scheduler_config=None,\n",
    "    optimizer_config=OptimizerConfig(\n",
    "        name=\"sgd\", arguments={\"lr\": 0.001, \"momentum\": 0.1}\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Define the model configuration object\n",
    "model_config = LenetConfig(name=\"lenet5\")\n",
    "\n",
    "# Define the Main parallel running configuration object\n",
    "run_config = MyParallelConfig(\n",
    "    train_config=train_config,\n",
    "    model_config=model_config,\n",
    "    metrics_n_batches=200,\n",
    "    total_trials=5,\n",
    "    concurrent_trials=5,\n",
    "    optim_metrics={\"val_loss\": \"min\"},\n",
    "    optim_metric_name=\"val_loss\",\n",
    "    gpu_mb_per_experiment=1024,\n",
    "    device=\"cuda\",\n",
    "    search_space={\n",
    "        \"train_config.optimizer_config.arguments.momentum\": SearchSpace(\n",
    "            value_range=(\"0.01\", \"0.1\"), value_type=\"float\"\n",
    "        )\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K2guR18nMhy1"
   },
   "source": [
    "### Hyperparameter tuning\n",
    "\n",
    "In this demo, we will train the model with different `momentum` values for the `SGD` optimizer. To achieve this functionality, we specified these parameters in the configurations object above:\n",
    "\n",
    "- `search_space`: Specify the hyperparameters we want to try with different values. We can specify their names, value ranges and value types. Ablator will generate different values for each hyperparameter according to the metrics and algorithms\n",
    "- `total_trials`: Specify how many trials we will have for different hyperparameters values.\n",
    "- `device`: Specify the hardware we will use to run our experiments\n",
    "\n",
    "Please refer to Ablator documentations for more information on how set the configurations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D6ztvvnI2WbW"
   },
   "source": [
    "## Set-up the model and datasets\n",
    "\n",
    "After we created our configurations, we can proceed and create our customized models and datasets.\n",
    "\n",
    "### Model implementations\n",
    "\n",
    "First, we define our customized model class. In this demo, we will use the LeNet-5 model defined by ourselves with each layer using PyTorch components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jYUj-kJZ2CDb"
   },
   "outputs": [],
   "source": [
    "# Customized Model class is defined here, where the model structure, forward pass\n",
    "# and loss function are defined\n",
    "\n",
    "\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.pool2 = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(16 * 4 * 4, 120)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.relu4 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool1(self.relu1(self.conv1(x)))\n",
    "        x = self.pool2(self.relu2(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 4 * 4)\n",
    "        x = self.relu3(self.fc1(x))\n",
    "        x = self.relu4(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self, config: LenetConfig) -> None:\n",
    "        super().__init__()\n",
    "        self.model = SimpleCNN()\n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "        # self.optimizer = optim.SGD(self.model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "    def forward(self, x, labels, custom_input=None):\n",
    "        # custom_input is for demo purposes only, defined in the dataset wrapper\n",
    "        out = self.model(x)\n",
    "        loss = self.loss(out, labels)\n",
    "        if labels is not None:\n",
    "            loss = self.loss(out, labels)\n",
    "\n",
    "        out = out.argmax(dim=-1)\n",
    "        return {\"y_pred\": out[:, None], \"y_true\": labels[:, None]}, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qnmgxa5zNSOU"
   },
   "source": [
    "### Datasets implementations\n",
    "\n",
    "Then, we will import the MNIST dataset and make dataloader out of it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Dac7fqrxNYC-"
   },
   "outputs": [],
   "source": [
    "# Create the training & validation dataloaders from the MNIST dataset.\n",
    "# Also, data preprocessing is defined here, including normalization and other transformations\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))]\n",
    ")\n",
    "\n",
    "trainset = torchvision.datasets.MNIST(\n",
    "    root=\"./datasets\", train=True, download=True, transform=transform\n",
    ")\n",
    "trainloader = torch.utils.data.DataLoader(\n",
    "    trainset, batch_size=64, shuffle=True, num_workers=2\n",
    ")\n",
    "\n",
    "testset = torchvision.datasets.MNIST(\n",
    "    root=\"./datasets\", train=False, download=True, transform=transform\n",
    ")\n",
    "testloader = torch.utils.data.DataLoader(\n",
    "    testset, batch_size=64, shuffle=False, num_workers=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ugidFvsINwTB"
   },
   "source": [
    "### Evaluation function implementations\n",
    "\n",
    "Also, we will define a evaluation function for training process and model evaluation. We will choose accuracy from the sklearn package as our metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T_GKbKheOELL"
   },
   "outputs": [],
   "source": [
    "def my_accuracy(y_true, y_pred):\n",
    "    return accuracy_score(y_true.flatten(), y_pred.flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H53BkivBOJir"
   },
   "source": [
    "### Final Wrap-up\n",
    "\n",
    "As a last step, we can wrap up the model, datasets and configurations into a wrapper class inheriting from ModelWrapper base class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zLLMX_CzPkry"
   },
   "outputs": [],
   "source": [
    "# Custom Model Wrapper, extending ModelWrapper class from Ablator\n",
    "class MyModelWrapper(ModelWrapper):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def make_dataloader_train(self, run_config: LenetRunConfig):  # type: ignore\n",
    "        return trainloader\n",
    "\n",
    "    def make_dataloader_val(self, run_config: LenetRunConfig):  # type: ignore\n",
    "        return testloader\n",
    "\n",
    "    def evaluation_functions(self) -> Dict[str, Callable]:\n",
    "        return {\"accuracy_score\": my_accuracy}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WbuGPUMmRBvk"
   },
   "source": [
    "## Launch Ablator\n",
    "\n",
    "After we finished the configurations and customizations, we can launch our Ablator to run the experiments now.\n",
    "\n",
    "The launching process follows these steps:\n",
    "\n",
    "*   Create target directory for results\n",
    "*   Initiate ray enviroments\n",
    "*   Run the experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UsZIiElKbmoo",
    "outputId": "bd55efff-88ae-4643-c740-65c124450ba0"
   },
   "outputs": [],
   "source": [
    "# Create results directory\n",
    "!mkdir -p working_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NL5WEPEe0eWo",
    "outputId": "2f71361b-2e60-498b-a258-d8268cf64720"
   },
   "outputs": [],
   "source": [
    "!ray stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GYqeT9Xh084-",
    "outputId": "3761c8cb-f42a-4f90-8b39-a7b93ddf5355"
   },
   "outputs": [],
   "source": [
    "# Debug to make sure your model can train just fine.\n",
    "\n",
    "import shutil\n",
    "\n",
    "EXPERIMENT_DIR = Path.cwd().joinpath(\"experiment_dir\")\n",
    "shutil.rmtree(EXPERIMENT_DIR, ignore_errors=True)\n",
    "run_config.experiment_dir = None\n",
    "\n",
    "wrapper = MyModelWrapper(\n",
    "    model_class=MyModel,\n",
    ")\n",
    "wrapper.train(run_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "FSZo_C1bRG1l",
    "outputId": "995fb941-6a3e-4694-f542-d97cef7cb964"
   },
   "outputs": [],
   "source": [
    "# Launch Ablator to run experiments\n",
    "\n",
    "\n",
    "WORKING_DIRECTORY = Path.cwd().joinpath(\"working_dir\")\n",
    "# mp_train prepares and launches parallel training\n",
    "\n",
    "wrapper = MyModelWrapper(\n",
    "    model_class=MyModel,\n",
    ")\n",
    "shutil.rmtree(EXPERIMENT_DIR, ignore_errors=True)\n",
    "run_config.experiment_dir = EXPERIMENT_DIR\n",
    "\n",
    "ablator = ParallelTrainer(\n",
    "    wrapper=wrapper,\n",
    "    run_config=run_config,\n",
    ")\n",
    "\n",
    "# NOTE to run on a cluster you will need to start ray with `ray start --head` and pass ray_head_address=\"auto\"\n",
    "ablator.launch(working_directory=WORKING_DIRECTORY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r3apIT0I4YJ7"
   },
   "source": [
    "## Experiments results analysis\n",
    "\n",
    "After running the experiments, the results are cached in the directory: `/tmp/dir`, as specified in the configurations. The results directory follows these structures:\n",
    "\n",
    "```\n",
    "- dir\n",
    "    - experiment_<experiment_id>\n",
    "        - <trial1_id>\n",
    "          - best_checkpoints/\n",
    "          - checkpoints/\n",
    "          - dashboard/\n",
    "          - config.yaml\n",
    "          - metadata.json\n",
    "          - results.json\n",
    "          - train.log\n",
    "        - <trial2_id>\n",
    "        - <trial3_id>\n",
    "        - ...\n",
    "        - <experiment_id>_optuna.db\n",
    "        - <experiment_id>_state.db\n",
    "        - master_config.yaml\n",
    "        - mp.log\n",
    "```\n",
    "\n",
    "To utilize the results, here are some detailed explations to introduce these files directories:\n",
    "\n",
    "- `master_config.yaml`: the overrall configurations for model, training and hyperparameters tuning\n",
    "- `train.log`: console infomation during the training process\n",
    "- `results.json`: metrics of the model during & after the training process\n",
    "- `config.yaml`: specific configurations for each trial, including the trail hyperparameters\n",
    "- `checkpoints/`: directory to cache the training checkpoints and trained models\n",
    "- `dashboard/`: directory to cache the metrics data for Tensorboard visualization\n",
    "\n",
    "In the folling section, we will use Tensorboard to visualize the results from different trials."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D6DaDY6NJ8c9"
   },
   "source": [
    "### Tensorboard visualization\n",
    "\n",
    "To utilize the Tensorboard, we load Tensorboard extension and then input each data directory into the Tensorboard and launch it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LCFbonfc4Ry6"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Load the TensorBoard extension\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3R3xHimZ5A-R"
   },
   "outputs": [],
   "source": [
    "from tensorboard import notebook\n",
    "\n",
    "# Load TensorBoard with multiple directories\n",
    "notebook.start(f\"--logdir {EXPERIMENT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N446HCexKxp7"
   },
   "source": [
    "### Results analysis\n",
    "\n",
    "Tensorboard gives us a clear visual on the performance of our model under different hyperparameter, to be specific, the momentum values of SGD optimizer.\n",
    "\n",
    "When the momentum is set to be `0.027` and `0.044`, the model can have a overrall best performance, both on the training set and on the validations set. Higher or lower momentum values may both lead to a poorer performance to our LeNet-5 model on the MNIST dataset."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
